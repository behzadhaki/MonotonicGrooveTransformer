{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/behzadhaki/MonotonicGrooveTransformer/blob/main/Monotonic_Groove_Transformer_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_BBV218LWCF"
      },
      "source": [
        "# **Demo - Transforming Monotonous Velocity Grooves using Transformer Neural Networks**\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEG3UNCoQoaq"
      },
      "source": [
        "## Environment setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "pt3b8qznRE_J",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#%%capture\n",
        "#@title Setup (Remove \"%%capture from top of cell\" to see output!)\n",
        "\n",
        "# !pip install -q condacolab\n",
        "# import condacolab\n",
        "# condacolab.install()\n",
        "\n",
        "\n",
        "# Installing magenta (for note_seq)\n",
        "!pip install -U -q magenta\n",
        "\n",
        "# Getting wandb\n",
        "!pip install -q wandb\n",
        "\n",
        "# Installing fluidsynth\n",
        "!apt-get update -qq && apt-get install -qq libfluidsynth1 fluid-soundfont-gm build-essential libasound2-dev libjack-dev\n",
        "!pip install -q pyfluidsynth\n",
        "import ctypes.util\n",
        "orig_ctypes_util_find_library = ctypes.util.find_library\n",
        "def proxy_find_library(lib):\n",
        "  if lib == 'fluidsynth':\n",
        "    return 'libfluidsynth.so.1'\n",
        "  else:\n",
        "    return orig_ctypes_util_find_library(lib)\n",
        "ctypes.util.find_library = proxy_find_library\n",
        "\n",
        "# Installing and activating environment\n",
        "#!conda env create -f TransformerGrooveTap2Drum/environment.yml\n",
        "\n",
        "from google.colab import files\n",
        "import IPython.display\n",
        "from IPython.display import Audio\n",
        "import magenta\n",
        "import note_seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9LXVRb2TZ0tS",
        "collapsed": true,
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Download Data and Source Code\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#@title\n",
        "# Cloning repository\n",
        "!git clone --quiet https://github.com/marinaniet0/TransformerGrooveTap2Drum\n",
        "\n",
        "# Unzipping dependencies\n",
        "!unzip -qq /content/TransformerVelGroove2Performance/dependencies.zip -d .\n",
        "\n",
        "# Unzipping midi data\n",
        "!unzip -qq /content/TransformerVelGroove2Performance/groove_midi_examples.zip -d .\n",
        "\n",
        "# Unzip trained models\n",
        "!unzip -qq /content/TransformerVelGroove2Performance/trained_models/misunderstood_bush_246-epoch_26.Model.zip -d .\n",
        "!unzip -qq /content/TransformerVelGroove2Performance/trained_models/rosy_durian_248-epoch_26.Model.zip -d .\n",
        "!unzip -qq /content/TransformerVelGroove2Performance/trained_models/solar_shadow_247-epoch_41.Model.zip -d .\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Import Libraries and Utilities\n",
        "\n",
        "from __future__ import print_function\n",
        "from ipywidgets import interact, interactive, fixed, interact_manual\n",
        "import ipywidgets as widgets\n",
        "import glob\n",
        "\n",
        "#@title Import libraries and define util functions\n",
        "import ipywidgets as widgets\n",
        "import os\n",
        "import torch\n",
        "import sys\n",
        "import note_seq\n",
        "import pretty_midi as pm\n",
        "import copy\n",
        "import wandb\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "sys.path.insert(1, \"/content/dependencies/BaseGrooveTransformers/\")\n",
        "sys.path.insert(1, \"/content/dependencies/hvo_sequence/\")\n",
        "\n",
        "from models.train import *\n",
        "from models.transformer import GrooveTransformerEncoder\n",
        "from hvo_sequence.drum_mappings import ROLAND_REDUCED_MAPPING\n",
        "from hvo_sequence.io_helpers import note_sequence_to_hvo_sequence\n",
        "from hvo_sequence.hvo_seq import empty_like\n",
        "\n",
        "def play(hvo_seq, sf2_path='/content/dependencies/hvo_sequence/hvo_sequence/soundfonts/Standard_Drum_Kit.sf2'):\n",
        "  audio_seq = hvo_seq.synthesize(sr=44100, sf_path=sf2_path)\n",
        "  IPython.display.display(IPython.display.Audio(audio_seq, rate=44100))\n",
        "\n",
        "def fixed_hvo_tsteps(hvo_arr, n_tsteps):\n",
        "  if hvo_arr.shape[0] > n_tsteps:\n",
        "    _hvo_arr = hvo_arr[:n_tsteps,:]\n",
        "  elif hvo_arr.shape[0] < n_tsteps:\n",
        "    _hvo_arr = np.concatenate((hvo_arr,np.zeros((n_tsteps-hvo_arr.shape[0], hvo_arr.shape[1]))))\n",
        "  else:\n",
        "    _hvo_arr = hvo_arr\n",
        "  return _hvo_arr\n",
        "  \n",
        "# find file names\n",
        "global file_names\n",
        "file_names = glob.glob(\"/content/groove_midi_examples/*4-4.mid\", recursive = True)\n",
        "\n",
        "\n",
        "def filename_interface(ID):\n",
        "  # for selecting midi files interactively down below\n",
        "  global file_name\n",
        "  file_name = file_names[ID]\n",
        "  return file_name"
      ],
      "metadata": {
        "cellView": "form",
        "id": "4UVaYKeolS7O",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Select Model Checkpoint**"
      ],
      "metadata": {
        "id": "m1jvxlqVg8qG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_filename = 'misunderstood_bush_246-epoch_26.Model' #@param [\"misunderstood_bush_246-epoch_26.Model\", \"rosy_durian_248-epoch_26.Model\", \"solar_shadow_247-epoch_41.Model\"]\n"
      ],
      "metadata": {
        "id": "VGZkpNMYg5_C",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load model\n",
        "%%capture\n",
        "TRAINED_MODELS_PATH = \"/content/\"\n",
        "\n",
        "params = {\n",
        "    'hopeful':{ 'd_model': 512, 'embedding_sz': 27, 'n_heads': 4,\n",
        "                      'dim_ff': 64, 'dropout': 0.1708, 'n_layers': 8,\n",
        "                      'max_len': 32, 'device': 'cpu' },\n",
        "    'misunderstood':{ 'd_model': 128, 'embedding_sz': 27, 'n_heads': 4,\n",
        "                          'dim_ff': 128, 'dropout': 0.1038, 'n_layers': 11,\n",
        "                          'max_len': 32, 'device': 'cpu' },\n",
        "    'rosy':{ 'd_model': 512, 'embedding_sz': 27, 'n_heads': 4,\n",
        "                    'dim_ff': 16, 'dropout': 0.1093, 'n_layers': 6,\n",
        "                    'max_len': 32, 'device': 'cpu' },\n",
        "    'solar':{ 'd_model': 128, 'embedding_sz': 27, 'n_heads': 1,\n",
        "                     'dim_ff': 16, 'dropout': 0.1594, 'n_layers': 7,\n",
        "                     'max_len': 32, 'device': 'cpu' }\n",
        "}\n",
        "\n",
        "selected_model_params = params[model_filename.split('_')[0]]\n",
        "\n",
        "# Load checkpoint\n",
        "checkpoint = torch.load(os.path.join(TRAINED_MODELS_PATH, model_filename),\n",
        "                        map_location=torch.device(selected_model_params['device']))\n",
        "\n",
        "# Initialize model\n",
        "groove_transformer = GrooveTransformerEncoder(selected_model_params['d_model'],\n",
        "                                              selected_model_params['embedding_sz'],\n",
        "                                              selected_model_params['embedding_sz'],\n",
        "                                              selected_model_params['n_heads'],\n",
        "                                              selected_model_params['dim_ff'],\n",
        "                                              selected_model_params['dropout'],\n",
        "                                              selected_model_params['n_layers'],\n",
        "                                              selected_model_params['max_len'],\n",
        "                                              selected_model_params['device'])\n",
        "# Load model and put in evaluation mode\n",
        "groove_transformer.load_state_dict(checkpoint['model_state_dict'])\n",
        "groove_transformer.eval()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "zEYhvH4mjKqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Select midi file\n",
        "interact(filename_interface, ID=int(len(file_names)/2));\n"
      ],
      "metadata": {
        "id": "FQFOq17KjFnw",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "KhDmbNr1xP1U"
      },
      "outputs": [],
      "source": [
        "#@title Tappify your own drum MIDI file or use an example from the Groove MIDI Dataset\n",
        "upload_myown_midi_file = False #@param {type:\"boolean\"}\n",
        "\n",
        "if upload_myown_midi_file:\n",
        "  uploaded_file = files.upload()\n",
        "  FILEPATH = list(uploaded.keys())[0]\n",
        "else:\n",
        "  FILEPATH = file_name\n",
        "  print(file_name)\n",
        "\n",
        "# Getting HVO representation\n",
        "gt_midi = pm.PrettyMIDI(FILEPATH)\n",
        "gt_note_seq = note_seq.midi_to_note_sequence(gt_midi)\n",
        "gt_hvo_seq = note_sequence_to_hvo_sequence(ns=gt_note_seq, drum_mapping=ROLAND_REDUCED_MAPPING)\n",
        "\n",
        "# Taking first 2 bars of file, padding with 0 if necessary\n",
        "gt_hvo_seq.hvo = fixed_hvo_tsteps(gt_hvo_seq.hvo, 32)\n",
        "\n",
        "tap_hvo_seq = copy.deepcopy(gt_hvo_seq)\n",
        "tap_hvo_seq.hvo = gt_hvo_seq.flatten_voices()\n",
        "\n",
        "print(\"Ground truth:\")\n",
        "play(gt_hvo_seq)\n",
        "print(\"Tappified:\")\n",
        "play(tap_hvo_seq)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "8cu0MBIy6oiM"
      },
      "outputs": [],
      "source": [
        "#@title Generate prediction from tapped input\n",
        "\n",
        "# hit_activation = \"use_probability_distribution\" #@param [\"use_threshold\", \"use_probability_distribution\"]\n",
        "hit_activation_threshold = 0 #@param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "\n",
        "# tapped sequence to tensor\n",
        "tap_hvo_tensor = torch.FloatTensor(tap_hvo_seq.hvo)\n",
        "\n",
        "\n",
        "#if hit_activation == \"use_threshold\":\n",
        "pred_h, pred_v, pred_o = groove_transformer.predict(\n",
        "  tap_hvo_tensor, use_thres=True, thres=hit_activation_threshold)\n",
        "#else:\n",
        "#  pred_h, pred_v, pred_o = groove_transformer.predict(\n",
        "#    tap_hvo_tensor, use_thres=False, use_pd=True)\n",
        "\n",
        "prediction_hvo_seq = empty_like(tap_hvo_seq)\n",
        "prediction_hvo_seq.hvo = np.zeros((32, 27))\n",
        "prediction_hvo_seq.hits = pred_h.numpy()[0]\n",
        "prediction_hvo_seq.velocities = pred_v.numpy()[0]\n",
        "prediction_hvo_seq.offsets = pred_o.numpy()[0]\n",
        "\n",
        "print(\"Tapped sequence:\")\n",
        "play(tap_hvo_seq)\n",
        "print(\"Generated beat:\")\n",
        "play(prediction_hvo_seq)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "nEpFhqagbuMb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Monotonic Groove Transformer Demo",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}